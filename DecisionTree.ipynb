{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your training file (second argument must be a .csv!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import operator\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "import ast\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "##################################################\n",
    "# data class to hold csv data\n",
    "##################################################\n",
    "class data():\n",
    "    def __init__(self, classifier):\n",
    "        self.examples = []\n",
    "        self.attributes = []\n",
    "        self.attr_types = []\n",
    "        self.classifier = classifier\n",
    "        self.class_index = None\n",
    "\n",
    "##################################################\n",
    "# function to read in data from the .csv files\n",
    "##################################################\n",
    "def read_data(dataset, datafile, datatypes):\n",
    "    print (\"Reading data...\")\n",
    "    f = open(datafile)\n",
    "    original_file = f.read()\n",
    "    rowsplit_data = original_file.splitlines()\n",
    "    dataset.examples = [rows.split(',') for rows in rowsplit_data]\n",
    "\n",
    "    #list attributes\n",
    "    dataset.attributes = dataset.examples.pop(0)\n",
    "\n",
    "    \n",
    "    #create array that indicates whether each attribute is a numerical value or not\n",
    "    attr_type = open(datatypes) \n",
    "    orig_file = attr_type.read()\n",
    "    dataset.attr_types = orig_file.split(',')\n",
    "\n",
    "##################################################\n",
    "# Preprocess dataset\n",
    "##################################################\n",
    "def preprocess2(dataset):\n",
    "    print (\"Preprocessing data...\")\n",
    "\n",
    "    class_values = [example[dataset.class_index] for example in dataset.examples]\n",
    "    class_mode = Counter(class_values)\n",
    "    class_mode = class_mode.most_common(1)[0][0]\n",
    "                         \n",
    "    for attr_index in range(len(dataset.attributes)):\n",
    "\n",
    "        ex_0class = filter(lambda x: x[dataset.class_index] == '0', dataset.examples)\n",
    "        values_0class = [example[attr_index] for example in ex_0class]  \n",
    "                           \n",
    "        ex_1class = filter(lambda x: x[dataset.class_index] == '1', dataset.examples)\n",
    "        values_1class = [example[attr_index] for example in ex_1class]\n",
    "                \n",
    "        values = Counter(values_0class)\n",
    "        value_counts = values.most_common()\n",
    "        \n",
    "        mode0 = values.most_common(1)[0][0]\n",
    "        if mode0 == '?':\n",
    "            mode0 = values.most_common(2)[1][0]\n",
    "\n",
    "        values = Counter(values_1class)\n",
    "        mode1 = values.most_common(1)[0][0]\n",
    "        \n",
    "        if mode1 == '?':\n",
    "            mode1 = values.most_common(2)[1][0]\n",
    "\n",
    "        mode_01 = [mode0, mode1]\n",
    "\n",
    "        attr_modes = [0]*len(dataset.attributes)\n",
    "        attr_modes[attr_index] = mode_01\n",
    "        \n",
    "        for example in dataset.examples:\n",
    "            if (example[attr_index] == '?'):\n",
    "                if (example[dataset.class_index] == '0'):\n",
    "                    example[attr_index] = attr_modes[attr_index][0]\n",
    "                elif (example[dataset.class_index] == '1'):\n",
    "                    example[attr_index] = attr_modes[attr_index][1]\n",
    "                else:\n",
    "                    example[attr_index] = class_mode\n",
    "\n",
    "        #convert attributes that are numeric to floats\n",
    "        for example in dataset.examples:\n",
    "            for x in range(len(dataset.examples[0])):\n",
    "                if dataset.attributes[x] == 'True':\n",
    "                    example[x] = float(example[x])\n",
    "\n",
    "##################################################\n",
    "# tree node class that will make up the tree\n",
    "##################################################\n",
    "class treeNode():\n",
    "    def __init__(self, is_leaf, classification, attr_split_index, attr_split_value, parent, upper_child, lower_child, height):\n",
    "        self.is_leaf = True\n",
    "        self.classification = None\n",
    "        self.attr_split = None\n",
    "        self.attr_split_index = None\n",
    "        self.attr_split_value = None\n",
    "        self.parent = parent\n",
    "        self.upper_child = None\n",
    "        self.lower_child = None\n",
    "        self.height = None\n",
    "\n",
    "##################################################\n",
    "# compute tree recursively\n",
    "##################################################\n",
    "\n",
    "# initialize Tree\n",
    "    # if dataset is pure (all one result) or there is other stopping criteria then stop\n",
    "    # for all attributes a in dataset\n",
    "        # compute information-theoretic criteria if we split on a\n",
    "    # abest = best attribute according to above\n",
    "    # tree = create a decision node that tests abest in the root\n",
    "    # dv (v=1,2,3,...) = induced sub-datasets from D based on abest\n",
    "    # for all dv\n",
    "        # tree = compute_tree(dv)\n",
    "        # attach tree to the corresponding branch of Tree\n",
    "    # return tree \n",
    "\n",
    "def compute_tree(dataset, parent_node, classifier):\n",
    "    node = treeNode(True, None, None, None, parent_node, None, None, 0)\n",
    "    if (parent_node == None):\n",
    "        node.height = 0\n",
    "    else:\n",
    "        node.height = node.parent.height + 1\n",
    "\n",
    "    ones = one_count(dataset.examples, dataset.attributes, classifier)\n",
    "    if (len(dataset.examples) == ones):\n",
    "        node.classification = 1\n",
    "        node.is_leaf = True\n",
    "        return node\n",
    "    elif (ones == 0):\n",
    "        node.classification = 0\n",
    "        node.is_leaf = True\n",
    "        return node\n",
    "    else:\n",
    "        node.is_leaf = False\n",
    "    attr_to_split = None # The index of the attribute we will split on\n",
    "    max_gain = 0 # The gain given by the best attribute\n",
    "    split_val = None \n",
    "    min_gain = 0.01\n",
    "    dataset_entropy = calc_dataset_entropy(dataset, classifier)\n",
    "    for attr_index in range(len(dataset.examples[0])):\n",
    "\n",
    "        if (dataset.attributes[attr_index] != classifier):\n",
    "            local_max_gain = 0\n",
    "            local_split_val = None\n",
    "            attr_value_list = [example[attr_index] for example in dataset.examples] # these are the values we can split on, now we must find the best one\n",
    "            attr_value_list = list(set(attr_value_list)) # remove duplicates from list of all attribute values\n",
    "            if(len(attr_value_list) > 100):\n",
    "                attr_value_list = sorted(attr_value_list)\n",
    "                total = len(attr_value_list)\n",
    "                ten_percentile = int(total/10)\n",
    "                new_list = []\n",
    "                for x in range(1, 10):\n",
    "                    new_list.append(attr_value_list[x*ten_percentile])\n",
    "                attr_value_list = new_list\n",
    "\n",
    "            for val in attr_value_list:\n",
    "                # calculate the gain if we split on this value\n",
    "                # if gain is greater than local_max_gain, save this gain and this value\n",
    "                local_gain = calc_gain(dataset, dataset_entropy, val, attr_index) # calculate the gain if we split on this value\n",
    "  \n",
    "                if (local_gain > local_max_gain):\n",
    "                    local_max_gain = local_gain\n",
    "                    local_split_val = val\n",
    "\n",
    "            if (local_max_gain > max_gain):\n",
    "                max_gain = local_max_gain\n",
    "                split_val = local_split_val\n",
    "                attr_to_split = attr_index\n",
    "\n",
    "    #attr_to_split is now the best attribute according to our gain metric\n",
    "    if (split_val is None or attr_to_split is None):\n",
    "        print( \"Something went wrong. Couldn't find an attribute to split on or a split value.\")\n",
    "    elif (max_gain <= min_gain or node.height > 20):\n",
    "\n",
    "        node.is_leaf = True\n",
    "        node.classification = classify_leaf(dataset, classifier)\n",
    "\n",
    "        return node\n",
    "\n",
    "    node.attr_split_index = attr_to_split\n",
    "    node.attr_split = dataset.attributes[attr_to_split]\n",
    "    node.attr_split_value = split_val\n",
    "    # currently doing one split per node so only two datasets are created\n",
    "    upper_dataset = data(classifier)\n",
    "    lower_dataset = data(classifier)\n",
    "    upper_dataset.attributes = dataset.attributes\n",
    "    lower_dataset.attributes = dataset.attributes\n",
    "    upper_dataset.attr_types = dataset.attr_types\n",
    "    lower_dataset.attr_types = dataset.attr_types\n",
    "    for example in dataset.examples:\n",
    "        if (attr_to_split is not None and example[attr_to_split] >= split_val):\n",
    "            upper_dataset.examples.append(example)\n",
    "        elif (attr_to_split is not None):\n",
    "            lower_dataset.examples.append(example)\n",
    "\n",
    "    node.upper_child = compute_tree(upper_dataset, node, classifier)\n",
    "    node.lower_child = compute_tree(lower_dataset, node, classifier)\n",
    "\n",
    "    return node\n",
    "\n",
    "##################################################\n",
    "# Classify dataset\n",
    "##################################################\n",
    "def classify_leaf(dataset, classifier):\n",
    "    ones = one_count(dataset.examples, dataset.attributes, classifier)\n",
    "    total = len(dataset.examples)\n",
    "    zeroes = total - ones\n",
    "    if (ones >= zeroes):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "##################################################\n",
    "# Calculate the entropy of the current dataset\n",
    "##################################################\n",
    "def calc_dataset_entropy(dataset, classifier):\n",
    "    ones = one_count(dataset.examples, dataset.attributes, classifier)\n",
    "    total_examples = len(dataset.examples);\n",
    "\n",
    "    entropy = 0\n",
    "    p = ones / total_examples\n",
    "    if (p != 0):\n",
    "        entropy += p * math.log(p, 2)\n",
    "    p = (total_examples - ones)/total_examples\n",
    "    if (p != 0):\n",
    "        entropy += p * math.log(p, 2)\n",
    "\n",
    "    entropy = -entropy\n",
    "    return entropy\n",
    "\n",
    "##################################################\n",
    "# Calculate the gain of a particular attribute split\n",
    "##################################################\n",
    "def calc_gain(dataset, entropy, val, attr_index):\n",
    "    classifier = dataset.attributes[attr_index]\n",
    "    attr_entropy = 0\n",
    "    total_examples = len(dataset.examples);\n",
    "    gain_upper_dataset = data(classifier)\n",
    "    gain_lower_dataset = data(classifier)\n",
    "    gain_upper_dataset.attributes = dataset.attributes\n",
    "    gain_lower_dataset.attributes = dataset.attributes\n",
    "    gain_upper_dataset.attr_types = dataset.attr_types\n",
    "    gain_lower_dataset.attr_types = dataset.attr_types\n",
    "    for example in dataset.examples:\n",
    "        if (example[attr_index] >= val):\n",
    "            gain_upper_dataset.examples.append(example)\n",
    "        elif (example[attr_index] < val):\n",
    "            gain_lower_dataset.examples.append(example)\n",
    "\n",
    "    if (len(gain_upper_dataset.examples) == 0 or len(gain_lower_dataset.examples) == 0): #Splitting didn't actually split (we tried to split on the max or min of the attribute's range)\n",
    "        return -1\n",
    "\n",
    "    attr_entropy += calc_dataset_entropy(gain_upper_dataset, classifier)*len(gain_upper_dataset.examples)/total_examples\n",
    "    attr_entropy += calc_dataset_entropy(gain_lower_dataset, classifier)*len(gain_lower_dataset.examples)/total_examples\n",
    "\n",
    "    return entropy - attr_entropy\n",
    "\n",
    "##################################################\n",
    "# count number of examples with classification \"1\"\n",
    "##################################################\n",
    "def one_count(instances, attributes, classifier):\n",
    "    count = 0\n",
    "    class_index = None\n",
    "    #find index of classifier\n",
    "    for a in range(len(attributes)):\n",
    "        if attributes[a] == classifier:\n",
    "            class_index = a\n",
    "        else:\n",
    "            class_index = len(attributes) - 1\n",
    "    for i in instances:\n",
    "        if i[class_index] == \"1\":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "##################################################\n",
    "# Prune tree\n",
    "##################################################\n",
    "def prune_tree(root, node, dataset, best_score):\n",
    "    # if node is a leaf\n",
    "    if (node.is_leaf == True):\n",
    "        # get its classification\n",
    "        classification = node.classification\n",
    "        # run validate_tree on a tree with the nodes parent as a leaf with its classification\n",
    "        node.parent.is_leaf = True\n",
    "        node.parent.classification = node.classification\n",
    "        if (node.height < 20):\n",
    "            new_score = validate_tree(root, dataset)\n",
    "        else:\n",
    "            new_score = 0\n",
    "  \n",
    "        # if its better, change it\n",
    "        if (new_score >= best_score):\n",
    "            return new_score\n",
    "        else:\n",
    "            node.parent.is_leaf = False\n",
    "            node.parent.classification = None\n",
    "            return best_score\n",
    "    # if its not a leaf\n",
    "    else:\n",
    "        # prune tree(node.upper_child)\n",
    "        new_score = prune_tree(root, node.upper_child, dataset, best_score)\n",
    "        # if its now a leaf, return\n",
    "        if (node.is_leaf == True):\n",
    "            return new_score\n",
    "        # prune tree(node.lower_child)\n",
    "        new_score = prune_tree(root, node.lower_child, dataset, new_score)\n",
    "        # if its now a leaf, return\n",
    "        if (node.is_leaf == True):\n",
    "            return new_score\n",
    "\n",
    "        return new_score\n",
    "\n",
    "##################################################\n",
    "# Validate tree\n",
    "##################################################\n",
    "def validate_tree(node, dataset):\n",
    "    total = len(dataset.examples)\n",
    "    correct = 0\n",
    "    for example in dataset.examples:\n",
    "        # validate example\n",
    "        correct += validate_example(node, example)\n",
    "    return correct/total\n",
    "\n",
    "##################################################\n",
    "# Validate example\n",
    "##################################################\n",
    "def validate_example(node, example):\n",
    "    if (node.is_leaf == True):\n",
    "        projected = node.classification\n",
    "        actual = int(example[-1])\n",
    "        if (projected == actual): \n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    value = example[node.attr_split_index]\n",
    "    if (value >= node.attr_split_value):\n",
    "        return validate_example(node.upper_child, example)\n",
    "    else:\n",
    "        return validate_example(node.lower_child, example)\n",
    "\n",
    "##################################################\n",
    "# Test example\n",
    "##################################################\n",
    "def test_example(example, node, class_index):\n",
    "    if (node.is_leaf == True):\n",
    "        return node.classification\n",
    "    else:\n",
    "        if (example[node.attr_split_index] >= node.attr_split_value):\n",
    "            return test_example(example, node.upper_child, class_index)\n",
    "        else:\n",
    "            return test_example(example, node.lower_child, class_index)\n",
    "\n",
    "##################################################\n",
    "# Print tree\n",
    "##################################################\n",
    "def print_tree(node):\n",
    "    if (node.is_leaf == True):\n",
    "        for x in range(node.height):\n",
    "            print (\"\\t\"),\n",
    "        print (\"Classification: \" + str(node.classification))\n",
    "        return\n",
    "    for x in range(node.height):\n",
    "            print (\"\\t\"),\n",
    "    print (\"Split index: \" + str(node.attr_split))\n",
    "    for x in range(node.height):\n",
    "            print (\"\\t\"),\n",
    "    print (\"Split value: \" + str(node.attr_split_value))\n",
    "    print_tree(node.upper_child)\n",
    "    print_tree(node.lower_child)\n",
    "\n",
    "##################################################\n",
    "# Print tree in disjunctive normal form\n",
    "##################################################\n",
    "def print_disjunctive(node, dataset, dnf_string):\n",
    "    if (node.parent == None):\n",
    "        dnf_string = \"( \"\n",
    "    if (node.is_leaf == True):\n",
    "        if (node.classification == 1):\n",
    "            dnf_string = dnf_string[:-3]\n",
    "            dnf_string += \") ^ \"\n",
    "            print (dnf_string,)\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        upper = dnf_string + str(dataset.attributes[node.attr_split_index]) + \" >= \" + str(node.attr_split_value) + \" V \"\n",
    "        print_disjunctive(node.upper_child, dataset, upper)\n",
    "        lower = dnf_string + str(dataset.attributes[node.attr_split_index]) + \" < \" + str(node.attr_split_value) + \" V \"\n",
    "        print_disjunctive(node.lower_child, dataset, lower)\n",
    "        return\n",
    "\n",
    "##################################################\n",
    "# main function, organize data and execute functions based on input\n",
    "# need to account for missing data\n",
    "##################################################\n",
    "\n",
    "def main():\n",
    "    args = str(sys.argv)\n",
    "    args = ast.literal_eval(args)\n",
    "    if (len(args) < 2):\n",
    "        print (\"You have input less than the minimum number of arguments. Go back and read README.txt and do it right next time!\")\n",
    "    elif (args[1][-4:] != \".csv\"):\n",
    "        print (\"Your training file (second argument must be a .csv!\")\n",
    "    else:\n",
    "        datafile = args[1]\n",
    "        dataset = data(\"\")\n",
    "        if (\"-d\" in args):\n",
    "            datatypes = args[args.index(\"-d\") + 1]\n",
    "        else:\n",
    "            datatypes = 'datatypes.csv'\n",
    "        read_data(dataset, datafile, datatypes)\n",
    "        arg3 = args[2]\n",
    "        if (arg3 in dataset.attributes):\n",
    "            classifier = arg3\n",
    "        else:\n",
    "            classifier = dataset.attributes[-1]\n",
    "\n",
    "        dataset.classifier = classifier\n",
    "\n",
    "        #find index of classifier\n",
    "        for a in range(len(dataset.attributes)):\n",
    "            if dataset.attributes[a] == dataset.classifier:\n",
    "                dataset.class_index = a\n",
    "            else:\n",
    "                dataset.class_index = range(len(dataset.attributes))[-1]\n",
    "                \n",
    "        unprocessed = copy.deepcopy(dataset)\n",
    "        preprocess2(dataset)\n",
    "\n",
    "        print (\"Computing tree...\")\n",
    "        root = compute_tree(dataset, None, classifier) \n",
    "        if (\"-s\" in args):\n",
    "            print_disjunctive(root, dataset, \"\")\n",
    "            print( \"\\n\")\n",
    "        if (\"-v\" in args):\n",
    "            datavalidate = args[args.index(\"-v\") + 1]\n",
    "            print (\"Validating tree...\")\n",
    "\n",
    "            validateset = data(classifier)\n",
    "            read_data(validateset, datavalidate, datatypes)\n",
    "            for a in range(len(dataset.attributes)):\n",
    "                if validateset.attributes[a] == validateset.classifier:\n",
    "                    validateset.class_index = a\n",
    "                else:\n",
    "                    validateset.class_index = range(len(validateset.attributes))[-1]\n",
    "            preprocess2(validateset)\n",
    "            best_score = validate_tree(root, validateset)\n",
    "            all_ex_score = copy.deepcopy(best_score)\n",
    "            print (\"Initial (pre-pruning) validation set score: \" + str(100*best_score) +\"%\")\n",
    "        if (\"-p\" in args):\n",
    "            if(\"-v\" not in args):\n",
    "                print (\"Error: You must validate if you want to prune\")\n",
    "            else:\n",
    "                post_prune_accuracy = 100*prune_tree(root, root, validateset, best_score)\n",
    "                print (\"Post-pruning score on validation set: \" + str(post_prune_accuracy) + \"%\")\n",
    "        if (\"-t\" in args):\n",
    "            datatest = args[args.index(\"-t\") + 1]\n",
    "            testset = data(classifier)\n",
    "            read_data(testset, datatest, datatypes)\n",
    "            for a in range(len(dataset.attributes)):\n",
    "                if testset.attributes[a] == testset.classifier:\n",
    "                    testset.class_index = a\n",
    "                else:\n",
    "                    testset.class_index = range(len(testset.attributes))[-1]\n",
    "            print (\"Testing model on \" + str(datatest))\n",
    "            for example in testset.examples:\n",
    "                example[testset.class_index] = '0'\n",
    "            testset.examples[0][testset.class_index] = '1'\n",
    "            testset.examples[1][testset.class_index] = '1'\n",
    "            testset.examples[2][testset.class_index] = '?'\n",
    "            preprocess2(testset)\n",
    "            b = open('results.csv', 'w')\n",
    "            a = csv.writer(b)\n",
    "            for example in testset.examples:\n",
    "                example[testset.class_index] = test_example(example, root, testset.class_index)\n",
    "            saveset = testset\n",
    "            saveset.examples = [saveset.attributes] + saveset.examples\n",
    "            a.writerows(saveset.examples)\n",
    "            b.close()\n",
    "            print (\"Testing complete. Results outputted to results.csv\")\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
